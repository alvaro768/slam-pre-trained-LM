{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SLAM_pre_trained_LM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KamJbKH6dzqN",
        "colab_type": "text"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8tpBPsSJYFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USE_TRAIN_AND_DEV = False # train with train set, evaluate with dev set\n",
        "USE_TRAIN_AND_DEV_AND_TEST = True # train with train and dev sets, evaluate with test set\n",
        "\n",
        "USE_LINEAR_LAYER = True\n",
        "USE_GRU_LAYER = False\n",
        "USE_BIGRU_LAYER = False\n",
        "USE_LSTM_LAYER = False\n",
        "USE_BILSTM_LAYER = False\n",
        "USE_BIGRU_LAYER_AND_DISTILBERT = False\n",
        "\n",
        "LINEAR_SIZE = 128\n",
        "GRU_SIZE = 128\n",
        "BIGRU_SIZE = 128\n",
        "LSTM_SIZE = 128\n",
        "BILSTM_SIZE = 128\n",
        "\n",
        "MAX_LEN = 21\n",
        "bs = 256\n",
        "LEARNING_RATE = 3e-4\n",
        "NUM_EPOCHS = 30\n",
        "NUM_TOKENS = 2373"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2Augtm0d559",
        "colab_type": "text"
      },
      "source": [
        "## Importing libraries & defining auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCGSn6enSFzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/'My Drive'/Thesis\n",
        "!ls\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm import tqdm, trange\n",
        "import pickle\n",
        "\n",
        "! pip install pytorch-transformers\n",
        "from pytorch_transformers.modeling_distilbert import *\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW3NVELgReW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import glob\n",
        "from copy import deepcopy\n",
        "from pandas.io.json import json_normalize\n",
        "from evalfixed import *\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn import BCELoss\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "pd.options.display.max_colwidth = 200\n",
        "pd.options.display.max_rows = 300\n",
        "\n",
        "%matplotlib inline\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfw4RSnsSb_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_transformers import DistilBertTokenizer, DistilBertConfig\n",
        "from pytorch_transformers import AdamW\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(n_gpu)\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0WblARqfTxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install seqeval\n",
        "from seqeval.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4iANNMhfa41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    preds_rounded = [0]*len(preds)\n",
        "\n",
        "    for i in range(len(preds)):\n",
        "        if preds[i] >= 0.5:\n",
        "            preds_rounded[i] = 1\n",
        "        else:\n",
        "            preds_rounded[i] = 0\n",
        "            \n",
        "    return np.sum(np.array(preds_rounded) == np.array(labels)) / len(preds_rounded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFLYYeiDmUqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "\n",
        "def compute_f1(actual, predicted):\n",
        "    \"\"\"\n",
        "    Computes the F1 score of your predictions. Note that we use 0.5 as the cutoff here.\n",
        "    \"\"\"\n",
        "    num = len(actual)\n",
        "\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "    true_negatives = 0\n",
        "\n",
        "    for i in range(num):\n",
        "        if actual[i] >= 0.5 and predicted[i] >= 0.5:\n",
        "            true_positives += 1\n",
        "        elif actual[i] < 0.5 and predicted[i] >= 0.5:\n",
        "            false_positives += 1\n",
        "        elif actual[i] >= 0.5 and predicted[i] < 0.5:\n",
        "            false_negatives += 1\n",
        "        else:\n",
        "            true_negatives += 1\n",
        "\n",
        "    try:\n",
        "        precision = true_positives / (true_positives + false_positives)\n",
        "        recall = true_positives / (true_positives + false_negatives)\n",
        "        F1 = 2 * precision * recall / (precision + recall)\n",
        "    except ZeroDivisionError:\n",
        "        F1 = 0.0\n",
        "\n",
        "    return F1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08OLGhsqgDBF",
        "colab_type": "text"
      },
      "source": [
        "## Defining models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZkzfluKgF8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyCustomDistilBertForTokenClassification(DistilBertPreTrainedModel):\n",
        "  \n",
        "  def __init__(self, config):\n",
        "        super(MyCustomDistilBertForTokenClassification, self).__init__(config)\n",
        "                  \n",
        "        \n",
        "        config.output_hidden_states=True # output all hidden states from all layers of DistilBert\n",
        "        self.output_hidden_states = True\n",
        "        \n",
        "        self.num_labels = config.num_labels\n",
        "        assert config.num_labels == 2\n",
        "        self.distilbert = DistilBertModel(config)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.dropout3 = nn.Dropout(0.3)\n",
        "        self.init_weights()\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        self.embedding_userID = nn.Embedding(num_embeddings=2593, embedding_dim=128)\n",
        "        self.embedding_format = nn.Embedding(num_embeddings=3, embedding_dim=64)\n",
        "        self.embedding_token = nn.Embedding(num_embeddings=NUM_TOKENS , embedding_dim=256)\n",
        "        \n",
        "        self.embedding_country = nn.Embedding(num_embeddings= 37, embedding_dim=64)\n",
        "        self.embedding_client = nn.Embedding(num_embeddings= 3, embedding_dim=64)\n",
        "        self.embedding_session = nn.Embedding(num_embeddings= 3, embedding_dim=64)\n",
        "        \n",
        "\n",
        "        if USE_LINEAR_LAYER:\n",
        "          self.linear = nn.Linear(256+128+1*2+64*4, LINEAR_SIZE)\n",
        "          self.linear_layer_2 = nn.Linear(LINEAR_SIZE, 32)\n",
        "\n",
        "        elif USE_GRU_LAYER:\n",
        "          self.gru_layer = nn.GRU(256+128+1*2+64*4, hidden_size=GRU_SIZE, num_layers=1, batch_first=True, bidirectional=False)\n",
        "          self.linear_layer_2 = nn.Linear(GRU_SIZE, 32)\n",
        "          \n",
        "        elif USE_BIGRU_LAYER:\n",
        "          self.bigru_layer = nn.GRU(256+128+1*2+64*4, hidden_size=BIGRU_SIZE, num_layers=1, batch_first=True, bidirectional=True)\n",
        "          self.linear_layer_2 = nn.Linear(BIGRU_SIZE*2, 32)\n",
        "\n",
        "        elif USE_LSTM_LAYER:\n",
        "          self.lstm_layer = nn.LSTM(256+128+1*2+64*4, hidden_size=LSTM_SIZE, num_layers=1, batch_first=True, bidirectional=False)\n",
        "          self.linear_layer_2 = nn.Linear(LSTM_SIZE, 32)\n",
        "\n",
        "        elif USE_BILSTM_LAYER:\n",
        "          self.bilstm_layer = nn.LSTM(256+128+1*2+64*4, hidden_size=BILSTM_SIZE, num_layers=1, batch_first=True, bidirectional=True)\n",
        "          self.linear_layer_2 = nn.Linear(BILSTM_SIZE*2, 32)\n",
        "\n",
        "        elif USE_BIGRU_LAYER_AND_DISTILBERT:\n",
        "          self.gru_layer = nn.GRU(256*2+256+128+1*2+64*4, hidden_size=256, num_layers=1, batch_first=True, bidirectional=True)\n",
        "          self.bert_gru = nn.GRU(config.hidden_size, hidden_size=256, num_layers=1, batch_first=True, bidirectional=True)\n",
        "          self.linear_layer_2 = nn.Linear(512, 32)\n",
        "\n",
        "\n",
        "        self.classifier = nn.Linear(32, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, input_ids, unique_exids, token, user_ids, ex_format, ex_days, country, client, session, time, token_type_ids=None, attention_mask=None, model_mask=None, head_mask=None, labels=None):\n",
        "\n",
        "        token_embedding = self.embedding_token(token)\n",
        "\n",
        "        user_ids = self.embedding_userID(user_ids)\n",
        "        ex_format = self.embedding_format(ex_format)\n",
        "        ex_days = ex_days[:,:,None]\n",
        "        time = time[:,:,None]\n",
        "        country = self.embedding_country(country)\n",
        "        client = self.embedding_client(client)\n",
        "        session = self.embedding_session(session)\n",
        "\n",
        "\n",
        "        if USE_LINEAR_LAYER:            \n",
        "            concat_output = torch.cat((token_embedding, user_ids, ex_format, ex_days, country, client, session, time),2)\n",
        "            concat_output = self.linear(concat_output)\n",
        "\n",
        "        elif USE_LSTM_LAYER:            \n",
        "            concat_output = torch.cat((token_embedding, user_ids, ex_format, ex_days, country, client, session, time),2)\n",
        "            concat_output, hidden = self.lstm(concat_output)\n",
        "\n",
        "        elif USE_BILSTM_LAYER:            \n",
        "            concat_output = torch.cat((token_embedding, user_ids, ex_format, ex_days, country, client, session, time),2)\n",
        "            concat_output, hidden = self.bilstm(concat_output)\n",
        "          \n",
        "        elif USE_GRU_LAYER:\n",
        "            concat_output = torch.cat((token_embedding, user_ids, ex_format, ex_days, country, client, session, time),2)\n",
        "            concat_output, hidden = self.gru_layer(concat_output)\n",
        "\n",
        "        elif USE_BIGRU_LAYER:\n",
        "            concat_output = torch.cat((token_embedding, user_ids, ex_format, ex_days, country, client, session, time),2)\n",
        "            concat_output, hidden = self.bigru_layer(concat_output)\n",
        "\n",
        "        elif USE_BIGRU_LAYER_AND_DISTILBERT:\n",
        "            bert_output = self.distilbert(input_ids, token_type_ids, head_mask=None)\n",
        "            bert_output = self.dropout3(bert_output[0]) # use to extract the last layer\n",
        "\n",
        "            #concat_bert = torch.cat((bert_output[-1][-1],bert_output[-1][-2]),2) # use to extract (Distil)BERT layers\n",
        "            bert_output, _ = self.bert_gru(bert_output)\n",
        "            \n",
        "            concat_output = torch.cat((bert_output, token_embedding, user_ids, ex_format, ex_days, country, client, session, time),2)\n",
        "\n",
        "\n",
        "        concat_output = self.dropout(concat_output)\n",
        "        concat_output = self.linear_layer_2(concat_output)\n",
        "        concat_output = self.relu(concat_output)\n",
        "        concat_output = self.dropout(concat_output)\n",
        "        \n",
        "        logits = self.classifier(concat_output)\n",
        "        logits = self.sigmoid(logits)      \n",
        "        \n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = BCELoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if model_mask is not None:\n",
        "                active_loss = model_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1)[active_loss]\n",
        "                active_labels = labels.view(-1)[active_loss]\n",
        "                \n",
        "                loss = loss_fct(active_logits, active_labels.float())\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, 1), labels.view(-1))\n",
        "            return loss\n",
        "        else:\n",
        "            return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DcxtvaIeE8_",
        "colab_type": "text"
      },
      "source": [
        "## Importing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYFrKzSSSG-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn = pd.read_pickle('trn.pkl')\n",
        "trn['exercise_id'] = trn['instance_id'].str[:-2]\n",
        "trn['days'] = (trn['days'] - np.mean(trn['days'])) / np.std(trn['days'])\n",
        "trn['time'].values[trn['time'] > 100] = 100\n",
        "trn['time'] = (trn['time'] - np.mean(trn['time'])) / np.std(trn['time'])\n",
        "\n",
        "dev = pd.read_pickle('dev.pkl')\n",
        "test = pd.read_pickle('test.pkl')\n",
        "\n",
        "dev['exercise_id'] = dev['instance_id'].str[:-2]\n",
        "test['exercise_id'] = test['instance_id'].str[:-2]\n",
        "\n",
        "dev['days'] = (dev['days'] - np.mean(dev['days'])) / np.std(dev['days'])\n",
        "test['days'] = (test['days'] - np.mean(test['days'])) / np.std(test['days'])\n",
        "\n",
        "dev['time'].values[dev['time'] > 100] = 100\n",
        "dev['time'] = (dev['time'] - np.mean(dev['time'])) / np.std(dev['time'])\n",
        "test['time'].values[test['time'] > 100] = 100\n",
        "test['time'] = (test['time'] - np.mean(test['time'])) / np.std(test['time'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GQzO0uTSNmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, p, q, r, t, v, n, l, f) for w, p, q, r, t, v, n, l, f in zip(s[\"token\"].values.tolist(),\n",
        "                                                                 s[\"label\"].values.tolist(),\n",
        "                                                                 s[\"user\"].values.tolist(),\n",
        "                                                                 s[\"format\"].values.tolist(),\n",
        "                                                                 s[\"days\"].values.tolist(),\n",
        "                                                                 s[\"countries\"].values.tolist(),\n",
        "                                                                 s[\"client\"].values.tolist(),\n",
        "                                                                 s[\"session\"].values.tolist(),\n",
        "                                                                 s[\"time\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"exercise_id\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PNMKGSvSUQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "getter_trn = SentenceGetter(trn)\n",
        "getter_dev = SentenceGetter(dev)\n",
        "getter_test = SentenceGetter(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HHLmED-SWgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens_trn = [[\"[CLS]\"] + [s[0] for s in sent] + [\"[SEP]\"] for sent in getter_trn.sentences]\n",
        "sentences_trn = [\"[CLS] \" + \" \".join([s[0] for s in sent]) + \" [SEP]\" for sent in getter_trn.sentences]\n",
        "labels_trn = [[0.0] + [s[1] for s in sent] + [0.0] for sent in getter_trn.sentences]\n",
        "userIDs_trn = [[sent[0][2]] + [s[2] for s in sent] + [sent[0][2]] for sent in getter_trn.sentences]\n",
        "format_trn = [[sent[0][3]] + [s[3] for s in sent] + [sent[0][3]] for sent in getter_trn.sentences]\n",
        "days_trn = [[sent[0][4]] + [s[4] for s in sent] + [sent[0][4]] for sent in getter_trn.sentences]\n",
        "country_trn = [[sent[0][5]] + [s[5] for s in sent] + [sent[0][5]] for sent in getter_trn.sentences]\n",
        "client_trn = [[sent[0][6]] + [s[6] for s in sent] + [sent[0][6]] for sent in getter_trn.sentences]\n",
        "session_trn = [[sent[0][7]] + [s[7] for s in sent] + [sent[0][7]] for sent in getter_trn.sentences]\n",
        "time_trn = [[sent[0][8]] + [s[8] for s in sent] + [sent[0][8]] for sent in getter_trn.sentences]\n",
        "\n",
        "\n",
        "tokens_dev = [[\"[CLS]\"] + [s[0] for s in sent] + [\"[SEP]\"] for sent in getter_dev.sentences]\n",
        "tokens_test = [[\"[CLS]\"] + [s[0] for s in sent] + [\"[SEP]\"] for sent in getter_test.sentences]\n",
        "print(tokens_dev[155])\n",
        "\n",
        "sentences_dev = [\"[CLS] \" + \" \".join([s[0] for s in sent]) + \" [SEP]\" for sent in getter_dev.sentences]\n",
        "sentences_test = [\"[CLS] \" + \" \".join([s[0] for s in sent]) + \" [SEP]\" for sent in getter_test.sentences]\n",
        "print(sentences_dev[155])\n",
        "\n",
        "labels_dev = [[0.0] + [s[1] for s in sent] + [0.0] for sent in getter_dev.sentences]\n",
        "labels_test = [[0.0] + [s[1] for s in sent] + [0.0] for sent in getter_test.sentences]\n",
        "print(labels_dev[155])\n",
        "\n",
        "userIDs_dev = [[sent[0][2]] + [s[2] for s in sent] + [sent[0][2]] for sent in getter_dev.sentences]\n",
        "userIDs_test = [[sent[0][2]] + [s[2] for s in sent] + [sent[0][2]] for sent in getter_test.sentences]\n",
        "print(userIDs_dev[155])\n",
        "\n",
        "format_dev = [[sent[0][3]] + [s[3] for s in sent] + [sent[0][3]] for sent in getter_dev.sentences]\n",
        "format_test = [[sent[0][3]] + [s[3] for s in sent] + [sent[0][3]] for sent in getter_test.sentences]\n",
        "print(format_dev[155])\n",
        "\n",
        "days_dev = [[sent[0][4]] + [s[4] for s in sent] + [sent[0][4]] for sent in getter_dev.sentences]\n",
        "days_test = [[sent[0][4]] + [s[4] for s in sent] + [sent[0][4]] for sent in getter_test.sentences]\n",
        "print(days_dev[155])\n",
        "\n",
        "country_dev = [[sent[0][5]] + [s[5] for s in sent] + [sent[0][5]] for sent in getter_dev.sentences]\n",
        "country_test = [[sent[0][5]] + [s[5] for s in sent] + [sent[0][5]] for sent in getter_test.sentences]\n",
        "print(country_dev[155])\n",
        "\n",
        "client_dev = [[sent[0][6]] + [s[6] for s in sent] + [sent[0][6]] for sent in getter_dev.sentences]\n",
        "client_test = [[sent[0][6]] + [s[6] for s in sent] + [sent[0][6]] for sent in getter_test.sentences]\n",
        "print(client_dev[155])\n",
        "\n",
        "session_dev = [[sent[0][7]] + [s[7] for s in sent] + [sent[0][7]] for sent in getter_dev.sentences]\n",
        "session_test = [[sent[0][7]] + [s[7] for s in sent] + [sent[0][7]] for sent in getter_test.sentences]\n",
        "print(session_dev[155])\n",
        "\n",
        "time_dev = [[sent[0][8]] + [s[8] for s in sent] + [sent[0][8]] for sent in getter_dev.sentences]\n",
        "time_test = [[sent[0][8]] + [s[8] for s in sent] + [sent[0][8]] for sent in getter_test.sentences]\n",
        "print(time_dev[155])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRUUxhs2w2uV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_sentences = sentences_trn + sentences_dev + sentences_test\n",
        "\n",
        "#assigning an index to each unique sentence\n",
        "sentence_corresponding_to_exercise = pd.factorize(all_sentences)[0]\n",
        "sentence_dictionary = list(pd.factorize(all_sentences)[1])\n",
        "\n",
        "#healthcheck\n",
        "print(all_sentences[34573])\n",
        "print(sentence_dictionary[sentence_corresponding_to_exercise[34573]])\n",
        "\n",
        "unique_ex_id_trn = [[sentence_corresponding_to_exercise[i]] + [sentence_corresponding_to_exercise[i] for s in sent] for i, sent in enumerate(getter_trn.sentences)]\n",
        "unique_ex_id_dev = [[sentence_corresponding_to_exercise[i+len(unique_ex_id_trn)]] + [sentence_corresponding_to_exercise[i+len(unique_ex_id_trn)] for s in sent] for i, sent in enumerate(getter_dev.sentences)]\n",
        "unique_ex_id_test = [[sentence_corresponding_to_exercise[i+len(unique_ex_id_trn)+len(unique_ex_id_dev)]] + [sentence_corresponding_to_exercise[i+len(unique_ex_id_trn)+len(unique_ex_id_dev)] for s in sent] for i, sent in enumerate(getter_test.sentences)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E45kJoOXo7zq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#healthcheck\n",
        "\n",
        "x, y, z = 3457, 4258, 435\n",
        "\n",
        "print(unique_ex_id_trn[x])\n",
        "print(unique_ex_id_dev[y])\n",
        "print(unique_ex_id_test[z])\n",
        "\n",
        "print(sentences_trn[x])\n",
        "print(sentence_dictionary[unique_ex_id_trn[x][0]])\n",
        "\n",
        "print(sentences_dev[y])\n",
        "print(sentence_dictionary[unique_ex_id_dev[y][0]])\n",
        "\n",
        "print(sentences_test[z])\n",
        "print(sentence_dictionary[unique_ex_id_test[z][0]])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ4vNvrTPe2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Get unique userIDs in order to calculate the embedding layer size\n",
        "\n",
        "unique_ids = set([u[0] for u in [[s[2] for s in sent] for sent in getter_trn.sentences]])\n",
        "\n",
        "  \n",
        "## Creating dictionaries for the variables\n",
        "\n",
        "#tokens\n",
        "if USE_TRAINING_DATA:\n",
        "    tokens_vals = [\"[CLS]\"] + [\"[SEP]\"] + list(set(list(set(trn[\"token\"].values)) + list(set(dev[\"token\"].values)) + list(set(test[\"token\"].values))))\n",
        "else:\n",
        "    tokens_vals = [\"[CLS]\"] + [\"[SEP]\"] + list(set(list(set(test[\"token\"].values)) + list(set(dev[\"token\"].values))))\n",
        "token2idx = {t: i for i, t in enumerate(tokens_vals)}\n",
        "\n",
        "#labels\n",
        "tags_vals = list(set(dev[\"label\"].values))\n",
        "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
        "\n",
        "#userID\n",
        "userid2idx = {u: i for i,u in enumerate(unique_ids)}\n",
        "\n",
        "#format\n",
        "format_vals = list(set(dev[\"format\"].values))\n",
        "format2idx = {t: i for i, t in enumerate(format_vals)}\n",
        "\n",
        "#country\n",
        "country_vals = list(set(dev[\"countries\"].values))\n",
        "country2idx = {t: i for i, t in enumerate(country_vals)}\n",
        "\n",
        "#client\n",
        "client_vals = list(set(dev[\"client\"].values))\n",
        "client2idx = {t: i for i, t in enumerate(client_vals)}\n",
        "\n",
        "#session\n",
        "session_vals = list(set(dev[\"session\"].values))\n",
        "session2idx = {t: i for i, t in enumerate(session_vals)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D25v_A_xHAZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Vocabulary size:\")\n",
        "len(tokens_vals)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJfeKU-5ShQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNCQ4B3gSiWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_texts_trn = [tokenizer.tokenize(sent) for sent in sentences_trn]\n",
        "input_ids_trn = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_trn],\n",
        "                      maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "tokenized_texts_dev = [tokenizer.tokenize(sent) for sent in sentences_dev]\n",
        "input_ids_dev = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_dev],\n",
        "                      maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "tokenized_texts_test = [tokenizer.tokenize(sent) for sent in sentences_test]\n",
        "input_ids_test = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_test],\n",
        "                      maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print(tokenized_texts_dev[0])\n",
        "print(input_ids_dev[0])\n",
        "\n",
        "input_tokens_trn = [[token2idx.get(t) for t in token] for token in tokens_trn]\n",
        "input_tokens_trn = pad_sequences(input_tokens_trn,\n",
        "                      maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "input_tokens_dev = [[token2idx.get(t) for t in token] for token in tokens_dev]\n",
        "input_tokens_dev = pad_sequences(input_tokens_dev,\n",
        "                      maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "input_tokens_test = [[token2idx.get(t) for t in token] for token in tokens_test]\n",
        "input_tokens_test = pad_sequences(input_tokens_test,\n",
        "                      maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print(input_tokens_dev[0])\n",
        "\n",
        "input_userids_trn = [[userid2idx.get(u) for u in uid] for uid in userIDs_trn]\n",
        "input_userids_trn = [[user[0]]*MAX_LEN for user in input_userids_trn]\n",
        "input_userids_dev = [[userid2idx.get(u) for u in uid] for uid in userIDs_dev]\n",
        "input_userids_dev = [[user[0]]*MAX_LEN for user in input_userids_dev]\n",
        "input_userids_test = [[userid2idx.get(u) for u in uid] for uid in userIDs_test]\n",
        "input_userids_test = [[user[0]]*MAX_LEN for user in input_userids_test]\n",
        "\n",
        "print(input_userids_dev[0])\n",
        "\n",
        "input_format_trn = [[format2idx.get(u) for u in uid] for uid in format_trn]\n",
        "input_format_trn = [[user[0]]*MAX_LEN for user in input_format_trn]\n",
        "input_format_dev = [[format2idx.get(u) for u in uid] for uid in format_dev]\n",
        "input_format_dev = [[user[0]]*MAX_LEN for user in input_format_dev]\n",
        "input_format_test = [[format2idx.get(u) for u in uid] for uid in format_test]\n",
        "input_format_test = [[user[0]]*MAX_LEN for user in input_format_test]\n",
        "\n",
        "print(input_format_dev[0])\n",
        "\n",
        "input_days_trn = [[days[0]]*MAX_LEN for days in days_trn]\n",
        "input_days_dev = [[days[0]]*MAX_LEN for days in days_dev]\n",
        "input_days_test = [[days[0]]*MAX_LEN for days in days_test]\n",
        "\n",
        "print(input_days_dev[0])\n",
        "\n",
        "input_country_trn = [[country2idx.get(l) for l in elem] for elem in country_trn]\n",
        "input_country_trn = [[elem[0]]*MAX_LEN for elem in input_country_trn]\n",
        "input_country_dev = [[country2idx.get(l) for l in elem] for elem in country_dev]\n",
        "input_country_dev = [[elem[0]]*MAX_LEN for elem in input_country_dev]\n",
        "input_country_test = [[country2idx.get(l) for l in elem] for elem in country_test]\n",
        "input_country_test = [[elem[0]]*MAX_LEN for elem in input_country_test]\n",
        "\n",
        "print(input_country_dev[0])\n",
        "\n",
        "input_client_trn = [[client2idx.get(l) for l in elem] for elem in client_trn]\n",
        "input_client_trn = [[elem[0]]*MAX_LEN for elem in input_client_trn]\n",
        "input_client_dev = [[client2idx.get(l) for l in elem] for elem in client_dev]\n",
        "input_client_dev = [[elem[0]]*MAX_LEN for elem in input_client_dev]\n",
        "input_client_test = [[client2idx.get(l) for l in elem] for elem in client_test]\n",
        "input_client_test = [[elem[0]]*MAX_LEN for elem in input_client_test]\n",
        "\n",
        "print(input_client_dev[0])\n",
        "\n",
        "input_session_trn = [[session2idx.get(l) for l in elem] for elem in session_trn]\n",
        "input_session_trn = [[elem[0]]*MAX_LEN for elem in input_session_trn]\n",
        "input_session_dev = [[session2idx.get(l) for l in elem] for elem in session_dev]\n",
        "input_session_dev = [[elem[0]]*MAX_LEN for elem in input_session_dev]\n",
        "input_session_test = [[session2idx.get(l) for l in elem] for elem in session_test]\n",
        "input_session_test = [[elem[0]]*MAX_LEN for elem in input_session_test]\n",
        "\n",
        "print(input_session_dev[0])\n",
        "\n",
        "input_time_trn = [[elem[0]]*MAX_LEN for elem in time_trn]\n",
        "input_time_dev = [[elem[0]]*MAX_LEN for elem in time_dev]\n",
        "input_time_test = [[elem[0]]*MAX_LEN for elem in time_test]\n",
        "\n",
        "print(input_time_dev[0])\n",
        "\n",
        "input_unique_exid_trn = [[exid[0]]*MAX_LEN for exid in unique_ex_id_trn]\n",
        "input_unique_exid_dev = [[exid[0]]*MAX_LEN for exid in unique_ex_id_dev]\n",
        "input_unique_exid_test = [[exid[0]]*MAX_LEN for exid in unique_ex_id_test]\n",
        "\n",
        "print(input_unique_exid_dev[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsQRI_spsUXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unique sentences prep\n",
        "\n",
        "tokenized_unique_sentences = [tokenizer.tokenize(sent) for sent in sentence_dictionary]\n",
        "input_ids_unique_sentences = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_unique_sentences],\n",
        "                      maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "attention_masks_unique_sentences = [[float(i>0) and float(i!=101) and float(i!=102) for i in ii] for ii in input_ids_unique_sentences]\n",
        "\n",
        "unique_sent_inputs = torch.tensor(input_ids_unique_sentences)\n",
        "unique_sent_masks = torch.tensor(attention_masks_unique_sentences)\n",
        "\n",
        "unique_sent_data = TensorDataset(unique_sent_inputs, unique_sent_masks)\n",
        "unique_sent_sampler = SequentialSampler(unique_sent_data)\n",
        "unique_sent_dataloader = DataLoader(unique_sent_data, sampler=unique_sent_sampler, batch_size=bs)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_ADSuhgS7yl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tags_trn = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_trn],\n",
        "                     maxlen=MAX_LEN, value=0.0, padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "tags_dev = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_dev],\n",
        "                     maxlen=MAX_LEN, value=0.0, padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "tags_test = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_test],\n",
        "                     maxlen=MAX_LEN, value=0.0, padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHK9FjpAS8K_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_masks_trn = [[float(i>1) for i in ii] for ii in input_tokens_trn] # 0 and 1 are CLS and SEP\n",
        "model_masks_dev = [[float(i>1) for i in ii] for ii in input_tokens_dev]\n",
        "model_masks_test = [[float(i>1) for i in ii] for ii in input_tokens_test]\n",
        "\n",
        "attention_masks_trn = [[float(i>0) for i in ii] for ii in input_ids_trn] \n",
        "attention_masks_dev = [[float(i>0) for i in ii] for ii in input_ids_dev]\n",
        "attention_masks_test = [[float(i>0) for i in ii] for ii in input_ids_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QafsKDpmCOm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # MERGE TRAIN AND DEV SETS \n",
        "\n",
        "input_ids_trndev = np.concatenate((input_ids_trn,input_ids_dev))\n",
        "input_unique_exid_trndev = np.concatenate((input_unique_exid_trn,input_unique_exid_dev))\n",
        "input_tokens_trndev = np.concatenate((input_tokens_trn,input_tokens_dev))\n",
        "input_userids_trndev = np.concatenate((input_userids_trn,input_userids_dev))\n",
        "input_format_trndev = np.concatenate((input_format_trn,input_format_dev))\n",
        "input_days_trndev = np.concatenate((input_days_trn,input_days_dev))\n",
        "input_country_trndev = np.concatenate((input_country_trn,input_country_dev))\n",
        "input_client_trndev = np.concatenate((input_client_trn,input_client_dev))\n",
        "input_session_trndev = np.concatenate((input_session_trn,input_session_dev))\n",
        "input_time_trndev = np.concatenate((input_time_trn,input_time_dev))\n",
        "tags_trndev = np.concatenate((tags_trn,tags_dev))\n",
        "attention_masks_trndev = np.concatenate((attention_masks_trn,attention_masks_dev))\n",
        "model_masks_trndev = np.concatenate((model_masks_trn,model_masks_dev))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUQNaBmtS_At",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if USE_TRAIN_AND_DEV:\n",
        "    tr_inputs = torch.tensor(input_ids_trn)\n",
        "    val_inputs = torch.tensor(input_ids_dev)\n",
        "    tr_unique_exid_inputs = torch.tensor(input_unique_exid_trn)\n",
        "    val_unique_exid_inputs = torch.tensor(input_unique_exid_dev)\n",
        "    tr_tokens = torch.tensor(input_tokens_trn)\n",
        "    val_tokens = torch.tensor(input_tokens_dev)\n",
        "    tr_userids = torch.tensor(input_userids_trn)\n",
        "    val_userids = torch.tensor(input_userids_dev)\n",
        "    tr_format = torch.tensor(input_format_trn)\n",
        "    val_format = torch.tensor(input_format_dev)\n",
        "    tr_days = torch.tensor(input_days_trn)\n",
        "    val_days = torch.tensor(input_days_dev)\n",
        "    tr_country = torch.tensor(input_country_trn)\n",
        "    val_country = torch.tensor(input_country_dev)\n",
        "    tr_client = torch.tensor(input_client_trn)\n",
        "    val_client = torch.tensor(input_client_dev)\n",
        "    tr_session = torch.tensor(input_session_trn)\n",
        "    val_session = torch.tensor(input_session_dev)\n",
        "    tr_time = torch.tensor(input_time_trn)\n",
        "    val_time = torch.tensor(input_time_dev)\n",
        "    tr_tags = torch.tensor(tags_trn)\n",
        "    val_tags = torch.tensor(tags_dev)\n",
        "    tr_masks = torch.tensor(attention_masks_trn)\n",
        "    val_masks = torch.tensor(attention_masks_dev)\n",
        "    tr_model_masks = torch.tensor(model_masks_trn)\n",
        "    val_model_masks = torch.tensor(model_masks_dev)\n",
        "    \n",
        "elif USE_TRAIN_AND_DEV_AND_TEST:\n",
        "    tr_inputs = torch.tensor(input_ids_trndev)\n",
        "    val_inputs = torch.tensor(input_ids_test)\n",
        "    tr_unique_exid_inputs = torch.tensor(input_unique_exid_trndev)\n",
        "    val_unique_exid_inputs = torch.tensor(input_unique_exid_test)\n",
        "    tr_tokens = torch.tensor(input_tokens_trndev)\n",
        "    val_tokens = torch.tensor(input_tokens_test)\n",
        "    tr_userids = torch.tensor(input_userids_trndev)\n",
        "    val_userids = torch.tensor(input_userids_test)\n",
        "    tr_format = torch.tensor(input_format_trndev)\n",
        "    val_format = torch.tensor(input_format_test)\n",
        "    tr_days = torch.tensor(input_days_trndev)\n",
        "    val_days = torch.tensor(input_days_test)\n",
        "    tr_country = torch.tensor(input_country_trndev)\n",
        "    val_country = torch.tensor(input_country_test)\n",
        "    tr_client = torch.tensor(input_client_trndev)\n",
        "    val_client = torch.tensor(input_client_test)\n",
        "    tr_session = torch.tensor(input_session_trndev)\n",
        "    val_session = torch.tensor(input_session_test)\n",
        "    tr_time = torch.tensor(input_time_trndev)\n",
        "    val_time = torch.tensor(input_time_test)\n",
        "    tr_tags = torch.tensor(tags_trndev)\n",
        "    val_tags = torch.tensor(tags_test)\n",
        "    tr_masks = torch.tensor(attention_masks_trndev)\n",
        "    val_masks = torch.tensor(attention_masks_test)\n",
        "    tr_model_masks = torch.tensor(model_masks_trndev)\n",
        "    val_model_masks = torch.tensor(model_masks_test)  \n",
        "\n",
        "train_data = TensorDataset(tr_inputs, tr_unique_exid_inputs, tr_tokens, tr_userids, tr_format, tr_days, tr_country, tr_client, tr_session, tr_time, tr_masks, tr_model_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_unique_exid_inputs, val_tokens, val_userids, val_format, val_days, val_country, val_client, val_session, val_time, val_masks, val_model_masks, val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKPUblIqfCH8",
        "colab_type": "text"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfm1skP2TBhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MyCustomDistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(tag2idx), output_hidden_states=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZNEcHQY9Tqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5wn94AjfxvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Healthcheck: compare tensors before & after training to check whether they have been updated\n",
        "\n",
        "before_embedding_userid = model.embedding_userID.weight.clone()\n",
        "before_embedding_format = model.embedding_format.weight.clone()\n",
        "before_embedding_token = model.embedding_token.weight.clone()\n",
        "before_embedding_bert_gru = model.bert_gru.all_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2g-6DMmzrGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Healthcheck: print layer 3\n",
        "\n",
        "for param in model.distilbert.transformer.layer[3].named_parameters():\n",
        "    print(param)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJOPexKOzuyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Healthcheck: print layer 5\n",
        "\n",
        "for param in model.distilbert.transformer.layer[5].named_parameters():\n",
        "    print(param)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXfZ7PzoTJo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finetuning all parameters\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "      'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "      'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = Adam(optimizer_grouped_parameters, lr=LEARNING_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiKhgBZcyu5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SELECT LAYERS WITHOUT FINETUNING\n",
        "\n",
        "for l in range(6):\n",
        "    for param in model.distilbert.transformer.layer[l].parameters():\n",
        "        param.requires_grad = False\n",
        "        \n",
        "for param in model.distilbert.embeddings.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUyaUdBBTfzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.cuda();\n",
        "\n",
        "epochs = NUM_EPOCHS\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "plot_train_loss = []\n",
        "plot_train_acc = []\n",
        "plot_val_loss = []\n",
        "plot_val_acc = []\n",
        "plot_val_auc = []\n",
        "plot_val_f1 = []\n",
        "\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    # TRAIN loop\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_unique_exids, b_token, b_userids, b_format, b_days, b_country, b_client, b_session, b_time, b_input_mask, b_model_mask, b_labels = batch\n",
        "\n",
        "        # forward pass\n",
        "        loss = model(b_input_ids, b_unique_exids, b_token, b_userids, b_format, b_days.float(), b_country, b_client, b_session, b_time.float(), token_type_ids=None, # .float() to fix error message \"float expected, got double\"\n",
        "                     attention_mask=b_input_mask, model_mask=b_model_mask, head_mask=None, labels=b_labels)\n",
        "        \n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        # track train loss\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        model.zero_grad()\n",
        "    # print train loss per epoch\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    plot_train_loss = plot_train_loss + [tr_loss/nb_tr_steps]\n",
        "    \n",
        "    \n",
        "    # VALIDATION \n",
        "    model.eval()\n",
        "    \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions , true_labels, actual_labels = [], [], []\n",
        "    \n",
        "    for batch in valid_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_unique_exids, b_token, b_userids, b_format, b_days, b_country, b_client, b_session, b_time, b_input_mask, b_model_mask, b_labels = batch\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            tmp_eval_loss = model(b_input_ids, b_unique_exids, b_token, b_userids, b_format, b_days, b_country, b_client, b_session, b_time, token_type_ids=None,\n",
        "                                  attention_mask=b_input_mask, model_mask=b_model_mask, head_mask=None, labels=b_labels)\n",
        "            logits = model(b_input_ids, b_unique_exids, b_token, b_userids, b_format, b_days, b_country, b_client, b_session, b_time, token_type_ids=None,\n",
        "                           attention_mask=b_input_mask, model_mask=b_model_mask, head_mask=None)\n",
        "            \n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        model_mask = b_model_mask.to('cpu').numpy()\n",
        "    \n",
        "        true_labels.append(label_ids)\n",
        "        \n",
        "        flattened_masks = flatten(model_mask)\n",
        "        flattened_labels = flatten(label_ids)\n",
        "        flattened_logits = flatten(flatten(logits))\n",
        "\n",
        "        zipped_vecs = zip(flattened_masks, flattened_labels, flattened_logits)\n",
        "        filtered_vecs = [(x, y, z) for x, y, z in zipped_vecs if x > 0]\n",
        "        filtered_masks, filtered_labels, filtered_logits = zip(*filtered_vecs)\n",
        "        \n",
        "        tmp_eval_accuracy = flat_accuracy(filtered_logits, filtered_labels)\n",
        "        \n",
        "        actual_labels.extend(filtered_labels)\n",
        "        predictions.extend(filtered_logits)\n",
        "        \n",
        "        eval_loss += tmp_eval_loss.mean().item()\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        \n",
        "        nb_eval_examples += b_input_ids.size(0)\n",
        "        nb_eval_steps += 1\n",
        "        \n",
        "    eval_loss = eval_loss/nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy/nb_eval_steps\n",
        "    \n",
        "    plot_val_loss = plot_val_loss + [eval_loss]\n",
        "    plot_val_acc = plot_val_acc + [eval_accuracy]\n",
        "    \n",
        "    print(\"Validation loss: {}\".format(eval_loss))\n",
        "    print(\"Validation Accuracy: {}\".format(eval_accuracy))\n",
        "    \n",
        "    metrics = evaluate_metrics(actual_labels, predictions)\n",
        "    loss_score = round(metrics['avglogloss'],4)\n",
        "    accuracy_score = round(metrics['accuracy'],4)\n",
        "    auc_score = round(metrics['auroc'], 4)\n",
        "    f1_score = round(metrics['F1'], 4)\n",
        "    \n",
        "    print(\"Validation AvgLogLoss: {}\".format(loss_score))\n",
        "    print(\"Validation Accuracy: {}\".format(accuracy_score))\n",
        "    print(\"Validation AUC: {}\".format(auc_score))\n",
        "    print(\"Validation F1-Score: {}\".format(f1_score))\n",
        "\n",
        "    plot_val_auc = plot_val_auc + [auc_score]\n",
        "    plot_val_f1 = plot_val_f1 + [f1_score]\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03G6CQ54_AOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Healthcheck: print layer 3\n",
        "\n",
        "for param in model.distilbert.transformer.layer[3].named_parameters():\n",
        "    print(param)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u46sJb0r-NfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Healthcheck: print layer 5\n",
        "\n",
        "for param in model.distilbert.transformer.layer[5].named_parameters():\n",
        "    print(param)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIwKY0t1F-NY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(10, 7))\n",
        "\n",
        "# \"Loss\"\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(plot_train_loss)\n",
        "plt.plot(plot_val_loss)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.yticks(np.arange(math.floor(min(plot_train_loss) * 10)/10.0, math.ceil(max(plot_val_loss)* 10)/10.0, 0.05))\n",
        "\n",
        "#  \"Accuracy\"\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(plot_val_acc)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['validation'], loc='upper left')\n",
        "\n",
        "#  \"AUC\"\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(plot_val_auc)\n",
        "plt.title('AUC score')\n",
        "plt.ylabel('score')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['AUC'], loc='upper left')\n",
        "\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(plot_val_f1)\n",
        "plt.title('F1-score')\n",
        "plt.ylabel('score')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['F1-score'], loc='upper left')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjkHETb3yint",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training model again (stack and finetune)\n",
        "\n",
        "LEARNING_RATE = 1e-6\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "      'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "      'weight_decay_rate': 0.0}\n",
        "]\n",
        "    \n",
        "optimizer = Adam(optimizer_grouped_parameters, lr=LEARNING_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQkaWpRx22hg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.cuda();\n",
        "\n",
        "epochs = NUM_EPOCHS\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "plot_train_loss = []\n",
        "plot_train_acc = []\n",
        "plot_val_loss = []\n",
        "plot_val_acc = []\n",
        "plot_val_auc = []\n",
        "plot_val_f1 = []\n",
        "\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    # TRAIN loop\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_unique_exids, b_token, b_userids, b_format, b_days, b_country, b_client, b_session, b_time, b_input_mask, b_labels = batch\n",
        "\n",
        "        # forward pass\n",
        "        loss = model(b_input_ids, b_unique_exids, b_token, b_userids, b_format, b_days.float(), b_country, b_client, b_session, b_time.float(), token_type_ids=None, # .float() to fix error message \"float expected, got double\"\n",
        "                     attention_mask=b_input_mask, labels=b_labels)\n",
        "        \n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        # track train loss\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        model.zero_grad()\n",
        "    # print train loss per epoch\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    plot_train_loss = plot_train_loss + [tr_loss/nb_tr_steps]\n",
        "    \n",
        "    \n",
        "    # VALIDATION on validation set\n",
        "    model.eval()\n",
        "    \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions , true_labels, actual_labels = [], [], []\n",
        "    \n",
        "    for batch in valid_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_unique_exids, b_token, b_userids, b_format, b_days, b_country, b_client, b_session, b_time, b_input_mask, b_labels = batch\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            tmp_eval_loss = model(b_input_ids, b_unique_exids, b_token, b_userids, b_format, b_days, b_country, b_client, b_session, b_time, token_type_ids=None,\n",
        "                                  attention_mask=b_input_mask, labels=b_labels)\n",
        "            logits = model(b_input_ids, b_unique_exids, b_token, b_userids, b_format, b_days, b_country, b_client, b_session, b_time, token_type_ids=None,\n",
        "                           attention_mask=b_input_mask)\n",
        "            \n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        input_mask = b_input_mask.to('cpu').numpy()\n",
        "    \n",
        "        true_labels.append(label_ids)\n",
        "        \n",
        "        flattened_masks = flatten(input_mask)\n",
        "        flattened_labels = flatten(label_ids)\n",
        "        flattened_logits = flatten(flatten(logits))\n",
        "\n",
        "        zipped_vecs = zip(flattened_masks, flattened_labels, flattened_logits)\n",
        "        filtered_vecs = [(x, y, z) for x, y, z in zipped_vecs if x > 0]\n",
        "        filtered_masks, filtered_labels, filtered_logits = zip(*filtered_vecs)\n",
        "        \n",
        "        tmp_eval_accuracy = flat_accuracy(filtered_logits, filtered_labels)\n",
        "        \n",
        "        actual_labels.extend(filtered_labels)\n",
        "        predictions.extend(filtered_logits)\n",
        "        \n",
        "        eval_loss += tmp_eval_loss.mean().item()\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        \n",
        "        nb_eval_examples += b_input_ids.size(0)\n",
        "        nb_eval_steps += 1\n",
        "        \n",
        "    eval_loss = eval_loss/nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy/nb_eval_steps\n",
        "    \n",
        "    plot_val_loss = plot_val_loss + [eval_loss]\n",
        "    plot_val_acc = plot_val_acc + [eval_accuracy]\n",
        "    \n",
        "    print(\"Validation loss: {}\".format(eval_loss))\n",
        "    print(\"Validation Accuracy: {}\".format(eval_accuracy))\n",
        "    \n",
        "    \n",
        "    metrics = evaluate_metrics(actual_labels, predictions)\n",
        "    loss_score = round(metrics['avglogloss'],4)\n",
        "    accuracy_score = round(metrics['accuracy'],4)\n",
        "    auc_score = round(metrics['auroc'], 4)\n",
        "    f1_score = round(metrics['F1'], 4)\n",
        "    \n",
        "    print(\"Validation AvgLogLoss: {}\".format(loss_score))\n",
        "    print(\"Validation Accuracy: {}\".format(accuracy_score))\n",
        "    print(\"Validation AUC: {}\".format(auc_score))\n",
        "    print(\"Validation F1-Score: {}\".format(f1_score))\n",
        "\n",
        "    plot_val_auc = plot_val_auc + [auc_score]\n",
        "    plot_val_f1 = plot_val_f1 + [f1_score]\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls2PDaI0DXhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(10, 7))\n",
        "\n",
        "# \"Loss\"\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(plot_train_loss)\n",
        "plt.plot(plot_val_loss)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.yticks(np.arange(math.floor(min(plot_train_loss) * 10)/10.0, math.ceil(max(plot_val_loss)* 10)/10.0, 0.05))\n",
        "\n",
        "#  \"Accuracy\"\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(plot_val_acc)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['validation'], loc='upper left')\n",
        "\n",
        "#  \"AUC\"\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(plot_val_auc)\n",
        "plt.title('AUC score')\n",
        "plt.ylabel('score')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['AUC'], loc='upper left')\n",
        "\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(plot_val_f1)\n",
        "plt.title('F1-score')\n",
        "plt.ylabel('score')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['F1-score'], loc='upper left')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3mnnLuhDWKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_avg_log_loss(actual, predicted):\n",
        "    \"\"\"\n",
        "    Computes the average log loss of your predictions.\n",
        "    \"\"\"\n",
        "    num = len(actual)\n",
        "    loss = 0.\n",
        "\n",
        "    for i in range(num):\n",
        "        p = predicted[i] if actual[i] > .5 else 1. - predicted[i]\n",
        "        print(str(p) + str(predicted[i]) + str(actual[i]))\n",
        "        loss -= math.log(p)\n",
        "    loss /= num\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZErSagI_ECr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compute_avg_log_loss(actual_labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR0b2o1cdjSH",
        "colab_type": "text"
      },
      "source": [
        "## Analysis of results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r54fUiTzAR96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(predictions)):\n",
        "  print(true_labels[0][i])\n",
        "  print(predictions[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S26n7yEZcwza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(label_ids)):\n",
        "  if 0 in label_ids[i]:\n",
        "      first_zero = np.where(label_ids[i] == 0)[0][0]\n",
        "      if 2 in np.argmax(logits, axis=2)[i][:first_zero]:\n",
        "          print(np.argmax(logits, axis=2)[i])\n",
        "          print(label_ids[i])\n",
        "          print(\" \")\n",
        "  else:\n",
        "      if 2 in np.argmax(logits, axis=2)[i][:first_zero]:\n",
        "          print(np.argmax(logits, axis=2)[i])\n",
        "          print(label_ids[i])\n",
        "          print(\" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zs5T4ymplLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.to('cpu')\n",
        "\n",
        "after_embedding_userid = model.embedding_userID.weight.clone()\n",
        "after_embedding_format = model.embedding_format.weight.clone()\n",
        "after_embedding_token = model.embedding_token.weight.clone()\n",
        "after_embedding_bert_gru = model.bert_gru.all_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8HI1-H2hg4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Distribution of actual errors\n",
        "\n",
        "n_words = 11\n",
        "results = [0]*n_words\n",
        "\n",
        "for row in labels_test:\n",
        "  row = row + [0]*(n_words-len(row))\n",
        "  results = [sum(x) for x in zip(results, row)]\n",
        "\n",
        "print(results)\n",
        "print(sum(results))\n",
        "\n",
        "results = results[1:]\n",
        "\n",
        "def plot_bar_x():\n",
        "    index = np.arange(len(results))+1\n",
        "    plt.bar(index, results)\n",
        "    plt.xlabel('Position in sentence', fontsize=10)\n",
        "    plt.ylabel('# of actual mistakes', fontsize=10)\n",
        "    plt.xticks(index, fontsize=10, rotation=30)\n",
        "    plt.show()\n",
        "    \n",
        "plot_bar_x()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNM709ywhj-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of sentences vs. Number of tokens\n",
        "\n",
        "n_sentences_per_length = [0]*20\n",
        "\n",
        "for row in labels_test:\n",
        "  n_sentences_per_length[len(row)-2-1] = n_sentences_per_length[len(row)-2-1] + 1 #-2 because of the start and end tags, -1 because of zero-indexing in python\n",
        "\n",
        "n_sentences_per_length = n_sentences_per_length[:10]\n",
        "\n",
        "print(n_sentences_per_length)\n",
        "\n",
        "def plot_bar_x():\n",
        "    index = np.arange(len(n_sentences_per_length))+1\n",
        "    plt.bar(index, n_sentences_per_length)\n",
        "    plt.xlabel('Sentence length', fontsize=10)\n",
        "    plt.ylabel('# of sentences', fontsize=10)\n",
        "    plt.xticks(index, fontsize=10, rotation=30)\n",
        "    plt.show()\n",
        "    \n",
        "plot_bar_x()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ot17nw52hk9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_tokens_per_position = [0]*10\n",
        "\n",
        "for i, x in enumerate(n_sentences_per_length):\n",
        "  n_tokens_per_position[i] = sum(n_sentences_per_length[i:])\n",
        "\n",
        "print(n_tokens_per_position)\n",
        "print(results)\n",
        "\n",
        "perc_actual_mistakes = [np.round(results[i]/n_tokens_per_position[i],3) for i, x in enumerate(results)]\n",
        "\n",
        "print(perc_actual_mistakes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IluLm4Xphosb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Distribution of actual errors (%)\n",
        "\n",
        "def plot_bar_x():\n",
        "    index = np.arange(len(perc_actual_mistakes))+1\n",
        "    plt.bar(index, perc_actual_mistakes)\n",
        "    plt.xlabel('Position in sentence', fontsize=10)\n",
        "    plt.ylabel('% of actual mistakes', fontsize=10)\n",
        "    plt.xticks(index, fontsize=10, rotation=30)\n",
        "    plt.yticks([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7], fontsize=10)\n",
        "    plt.show()\n",
        "    \n",
        "plot_bar_x()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srIp9niFhqxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_one_zero = [1 if x>=0.5 else 0 for x in predictions]\n",
        "predictions_backup = predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpG1HIuhhs2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform \n",
        "\n",
        "predictions_vec = predictions_one_zero\n",
        "\n",
        "predictions_list = [None]*len(labels_test)\n",
        "previous_idx = 0\n",
        "\n",
        "for i, row in enumerate(labels_test):\n",
        "  idx = previous_idx + len(row) - 2\n",
        "  predictions_list[i] = predictions_vec[previous_idx:idx]\n",
        "  previous_idx = idx\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzu5hC2lhusu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Distribution of predicted errors\n",
        "\n",
        "n_words = 10\n",
        "results = [0]*n_words\n",
        "\n",
        "for row in predictions_list:\n",
        "  row = row + [0]*(n_words-len(row))\n",
        "  results = [sum(x) for x in zip(results, row)]\n",
        "\n",
        "print(results)\n",
        "print(sum(results))\n",
        "\n",
        "def plot_bar_x():\n",
        "    index = np.arange(len(results))+1\n",
        "    plt.bar(index, results)\n",
        "    plt.xlabel('Position in sentence', fontsize=10)\n",
        "    plt.ylabel('# of predicted mistakes', fontsize=10)\n",
        "    plt.xticks(index, fontsize=10, rotation=30)\n",
        "    plt.show()\n",
        "    \n",
        "plot_bar_x()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFBQexfahwM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "perc_predicted_mistakes = [np.round(results[i]/n_tokens_per_position[i],3) for i, x in enumerate(results)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un-pPibYh1AJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Distribution of predicted errors (%)\n",
        "\n",
        "def plot_bar_x():\n",
        "    index = np.arange(len(perc_predicted_mistakes))+1\n",
        "    plt.bar(index, perc_predicted_mistakes)\n",
        "    plt.xlabel('Position in sentence', fontsize=10)\n",
        "    plt.ylabel('% of predicted mistakes', fontsize=10)\n",
        "    plt.xticks(index, fontsize=10, rotation=30)\n",
        "    plt.yticks([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7], fontsize=10)\n",
        "    plt.show()\n",
        "    \n",
        "plot_bar_x()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKlAlgN1iD_6",
        "colab_type": "text"
      },
      "source": [
        "### Error analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nREgFIeiiIRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_test_no_first_last = [vec[1:-1] for vec in labels_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0oxBsrGiKKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct_predictions = [0]*15\n",
        "wrong_predictions = [0]*15\n",
        "number_of_predictions = [0]*15\n",
        "\n",
        "for i, row in enumerate(predictions_list):\n",
        "  for j, prediction in enumerate(row):\n",
        "    number_of_predictions[j] = number_of_predictions[j] + 1\n",
        "    if prediction == labels_test_no_first_last[i][j]:\n",
        "      correct_predictions[j] = correct_predictions[j] + 1\n",
        "    elif prediction != labels_test_no_first_last[i][j]:\n",
        "      wrong_predictions[j] = wrong_predictions[j] + 1\n",
        "    else:\n",
        "      break\n",
        "\n",
        "print(correct_predictions[0:10])\n",
        "print(number_of_predictions[0:10])\n",
        "print(wrong_predictions[0:10])\n",
        "print(sum(correct_predictions) + sum(wrong_predictions) == sum(number_of_predictions))\n",
        "\n",
        "model_errors = [np.round(pred/number_of_predictions[i],3) for i, pred in enumerate(wrong_predictions[0:10])]\n",
        "print(model_errors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckwn_a-miLuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Distribution of model prediction errors (%)\n",
        "\n",
        "def plot_bar_x():\n",
        "    index = np.arange(len(model_errors))+1\n",
        "    plt.bar(index, model_errors)\n",
        "    plt.xlabel('Position in sentence', fontsize=10)\n",
        "    plt.ylabel('% of predictions wrong', fontsize=10)\n",
        "    plt.xticks(index, fontsize=10, rotation=30)\n",
        "    plt.yticks([0.0,0.1,0.2,0.3], fontsize=10)\n",
        "    plt.show()\n",
        "    \n",
        "plot_bar_x()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx_5R4fuiNPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true_positives = [0]*15\n",
        "false_positives = [0]*15\n",
        "true_negatives = [0]*15\n",
        "false_negatives = [0]*15\n",
        "number_of_predictions = [0]*15\n",
        "\n",
        "for i, row in enumerate(predictions_list):\n",
        "  for j, prediction in enumerate(row):\n",
        "    number_of_predictions[j] = number_of_predictions[j] + 1\n",
        "\n",
        "    if (prediction == 1) & (labels_test_no_first_last[i][j] == 1):\n",
        "      true_positives[j] = true_positives[j] + 1\n",
        "\n",
        "    elif (prediction == 1) & (labels_test_no_first_last[i][j] == 0):\n",
        "      false_positives[j] = false_positives[j] + 1\n",
        "\n",
        "    elif (prediction == 0) & (labels_test_no_first_last[i][j] == 1):\n",
        "      false_negatives[j] = false_negatives[j] + 1\n",
        "\n",
        "    elif (prediction == 0) & (labels_test_no_first_last[i][j] == 0):\n",
        "      true_negatives[j] = true_negatives[j] + 1\n",
        "\n",
        "    else:\n",
        "      break\n",
        "\n",
        "print(true_positives[0:10])\n",
        "print(false_positives[0:10])\n",
        "print(true_negatives[0:10])\n",
        "print(false_negatives[0:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcA1BeWxiPYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Distribution of model prediction errors\n",
        "\n",
        "def plot_bar_x():\n",
        "    fig, ax = plt.subplots(ncols=2, figsize=(15,4),)\n",
        "\n",
        "    index = np.arange(len(true_positives[0:10]))+1\n",
        "    ax[0].bar(index - 0.25, true_positives[0:10], width = 0.2, color = \"green\", label = \"True Positives\")\n",
        "    ax[0].bar(index + 0.00, false_positives[0:10], width = 0.2, label = \"False Positives\")\n",
        "    ax[0].bar(index + 0.25, false_negatives[0:10], width = 0.2, label = \"False Negatives\")\n",
        "    ax[0].legend()\n",
        "    ax[0].set_xlabel('Position in sentence', fontsize=10)\n",
        "    ax[0].set_ylabel('% of occurrences', fontsize=10)\n",
        "    ax[0].set_xticks(index)\n",
        "\n",
        "    ax[1].bar(index, true_negatives[0:10], width = 0.2, color = \"red\", label = \"True Negatives\")\n",
        "    ax[1].legend()\n",
        "    ax[1].set_xlabel('Position in sentence', fontsize=10)\n",
        "    ax[1].set_ylabel('% of occurrences', fontsize=10)\n",
        "    ax[1].set_xticks(index)\n",
        "\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "    \n",
        "    \n",
        "plot_bar_x()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH1K4ADTiYIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "perc_true_positives = [np.round(x/number_of_predictions[i],3) for i, x in enumerate(true_positives[0:10])]\n",
        "perc_false_positives = [np.round(x/number_of_predictions[i],3) for i, x in enumerate(false_positives[0:10])]\n",
        "perc_true_negatives = [np.round(x/number_of_predictions[i],3) for i, x in enumerate(true_negatives[0:10])]\n",
        "perc_false_negatives = [np.round(x/number_of_predictions[i],3) for i, x in enumerate(false_negatives[0:10])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYsSDH6NiZgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Distribution of model prediction errors (%)\n",
        "\n",
        "def plot_bar_x():\n",
        "    fig, ax = plt.subplots(ncols=2, figsize=(15,4),)\n",
        "\n",
        "    index = np.arange(len(true_positives[0:10]))+1\n",
        "    ax[0].bar(index - 0.25, perc_true_positives[0:10], width = 0.2, color = \"green\", label = \"True Positives\")\n",
        "    ax[0].bar(index + 0.00, perc_false_positives[0:10], width = 0.2, label = \"False Positives\")\n",
        "    ax[0].bar(index + 0.25, perc_false_negatives[0:10], width = 0.2, label = \"False Negatives\")\n",
        "    ax[0].legend()\n",
        "    ax[0].set_xlabel('Position in sentence', fontsize=10)\n",
        "    ax[0].set_ylabel('% of occurrences', fontsize=10)\n",
        "    ax[0].set_xticks(index)\n",
        "\n",
        "    ax[1].bar(index, perc_true_negatives[0:10], width = 0.2, color = \"red\", label = \"True Negatives\")\n",
        "    ax[1].legend()\n",
        "    ax[1].set_xlabel('Position in sentence', fontsize=10)\n",
        "    ax[1].set_ylabel('% of occurrences', fontsize=10)\n",
        "    ax[1].set_xticks(index)\n",
        "\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "    \n",
        "    \n",
        "plot_bar_x()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}